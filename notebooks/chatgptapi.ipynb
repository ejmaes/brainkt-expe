{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPT API\n",
    "Using ChatGPT API for various reasons - summaries, theme annotation, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import pandas as pd\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "from speach import elan\n",
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import IPython\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and parametrizing\n",
    "import openai\n",
    "import tiktoken # get the number of tokens: use tiktoken\n",
    "\n",
    "with open(\"../src/openai_apk\", 'r') as f:\n",
    "    d = {x.split('=')[0]:literal_eval(x.split('=')[1]) for x in f.readlines()}\n",
    "openai.api_key = d['API_KEY']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatGPT prompt / calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_tokens(s:str, model:str=\"gpt-3.5-turbo\"):\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\") # can use for other models\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    encs = enc.encode(s)\n",
    "    return len(encs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(s:str, speakers:list):\n",
    "    s = f\"\"\"ce texte est extrait d'une conversation entre {' et '.join(speakers)}. lorsqu'un nouveau locuteur prend la parole, son nom est indiqué entre crochets (par exemple <{speakers[0]}>). Peux-tu me donner les différents thèmes de la conversation et me citer la phrase par laquelle ils débutent ? par exemple: \"thème 1: [thème] (phrase d'introduction: [extrait du texte])\"\n",
    "\n",
    "conversation: \"{s}\" \"\"\"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_solution(conversation:str, speakers:list):\n",
    "    speaker = speakers[0]\n",
    "    return f\"\"\"Dans la conversation suivante, les locuteurs (indiqués entre crochets, par exemple <{speaker}>) cherchent à résoudre un dilemme moral : ils doivent choisir qui, parmi les passager d'une montgolfière, jeter par dessus bord pour sauver les autres. Que choisissent-ils ? . Répond en un mot parmi les propositions suivantes: \"scientifique\", \"maitresse\", \"pilote\", \"aucun\".\n",
    "\n",
    "conversation: \\\"{conversation}\\\"\"\"\"\n",
    "\n",
    "def create_prompt_group_themes(themes, mode:str='group'):\n",
    "    \"\"\"\n",
    "    themes arg: flatten themes found in parse_response; function not called if 1 answer is not parsed properly\n",
    "        [y for x in calls for y in parse_chatgpt_res(calls[x]['response'], 'all')[0]]\n",
    "    \"\"\"\n",
    "    auth_modes = ['group','dilemma']\n",
    "    if mode not in auth_modes:\n",
    "        raise ValueError(f\"Authorised modes are: {auth_modes}; currently {mode}\")\n",
    "    if mode == \"group\":\n",
    "        prompt = \"\"\"A partir de la liste des sujets ci-après, peux-tu me créer des groupes de sujets adjacents similaires ? Si deux sujets similaires sont séparés par un autre thème, alors le sujet au milieu appartient au même groupe que les deux autres (par exemple, si les sujets 1 et 3 appartiennent au même groupe, alors le thème 2 est dans le même groupe que les thèmes 1 et 3).\n",
    "        \n",
    "Les thèmes sont:\\n\"\"\"\n",
    "    elif mode == \"dilemma\":\n",
    "        prompt = \"\"\"Parmi les thèmes ci-après, lesquels sont liés à la résolution du dilemme moral du choix d'un passager d'une montgolfière à sacrifier ?\n",
    "    \n",
    "Les thèmes sont:\\n\"\"\"\n",
    "    for i, theme in enumerate(themes):\n",
    "        prompt += f\"{i+1}. {theme}\\n\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_api(s:str, model:str=\"gpt-3.5-turbo\"):#, max_tokens:int=256):\n",
    "    \"\"\"\n",
    "    Response:\n",
    "    {\n",
    "        'id': 'chatcmpl-6p9XYPYSTTRi0xEviKjjilqrWU2Ve',\n",
    "        'object': 'chat.completion',\n",
    "        'created': 1677649420,\n",
    "        'model': 'gpt-3.5-turbo',\n",
    "        'usage': {'prompt_tokens': 56, 'completion_tokens': 31, 'total_tokens': 87},\n",
    "        'choices': [\n",
    "        {\n",
    "            'message': {\n",
    "            'role': 'assistant',\n",
    "            'content': 'The 2020 World Series was played in Arlington, Texas at the Globe Life Field, which was the new home stadium for the Texas Rangers.'},\n",
    "            'finish_reason': 'stop', # 'length' (token limit), 'content_filter' (flagged), 'null' (still in progress)\n",
    "            'index': 0\n",
    "        }\n",
    "        ]\n",
    "    }\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=[ {\"role\": \"user\", \"content\": s} ],\n",
    "# Parametrizing not supported\n",
    "#        temperature=0.7,\n",
    "#        max_tokens=max_tokens,\n",
    "#        top_p=1,\n",
    "#        frequency_penalty=0,\n",
    "#        presence_penalty=0\n",
    "    )\n",
    "    res = response['choices'][0]['message']['content']\n",
    "    nb_tok = {'total': response['usage']['total_tokens'], 'response': response['usage']['completion_tokens']}\n",
    "    return res, nb_tok"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare text for call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcr_loc = \"../data/transcript/current\"\n",
    "transcr_files = sorted([x for x in os.listdir(transcr_loc) if 'eaf' in x])\n",
    "markers_path = \"../data/video/markers_from_video_start.csv\"\n",
    "markers = pd.read_csv(markers_path).set_index('file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_df(file:str):\n",
    "    _, date, group = file.split('.')[0].split('-')\n",
    "    eaf = elan.read_eaf(os.path.join(transcr_loc, file))\n",
    "    dial = pd.DataFrame(eaf.to_csv_rows(), columns=['speaker', '?', 'start','stop','duration','text'])\n",
    "    dial.start = dial.start.astype(float)\n",
    "    dial.stop = dial.stop.astype(float)\n",
    "    dial = dial[~dial.text.isin(['#', ''])].sort_values('start').reset_index(drop=True)\n",
    "    dial.drop(columns=['?'], inplace=True)\n",
    "    dial.speaker = dial.speaker.apply(lambda x: x.split('-')[-1])\n",
    "    mark = markers.loc[f'{date}_{group}']\n",
    "    fconv = dial[dial.start >= (mark['End Task 1'] + 2)] # giving seconds to start task\n",
    "    return dial, fconv\n",
    "\n",
    "def ann_overlap(df):\n",
    "    # df if ordered by start, stop. so the line before (more before if several during one ipu) should tell if \n",
    "    df['overlap'] = False\n",
    "    for idx, row in df.iterrows():\n",
    "        tmp = df.loc[(row.speaker != df.speaker) & (df.start <= row.start) & (df.stop >= row.stop)]\n",
    "        if tmp.shape[0] > 0:\n",
    "            df.loc[idx, 'overlap'] = True\n",
    "\n",
    "def is_feedback(s:str, threshold:float=.5) -> bool:\n",
    "    words = ['ouais','mh','hm', \"c'est sûr\", 'oui', 'ha','ah', '@', 'alors', 'voilà', 'ok', 'du coup']\n",
    "    nw = re.findall(re.compile('('+'|'.join(words)+')'), s.lower())\n",
    "    return (len(nw) / len(s.split())) > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_texts(df:pd.DataFrame, max_nb_tokens:int=2048, idx_add_after_breakpoint:int=1):\n",
    "    \"\"\"Max number of tokens for API: 2048/4000 - cannot use overlap since anwsers might not overlap\"\"\"\n",
    "    memory = []\n",
    "    # 1. get the number of tokens for every line in df\n",
    "    ### Which is the best option so as not to get almost empty last line?\n",
    "    df['text_updated'] = df.apply(lambda x: f\"<{x.speaker}> {x.text}\", axis=1)\n",
    "    #df['speaker_updated'] = (df.speaker != df.speaker.shift())*df.speaker\n",
    "    #df['text_updated'] = df.apply(lambda x: f\"<{x.speaker_updated}> {x.text}\" if x.speaker_updated != '' else x.text, axis=1)\n",
    "    df['nb_tok'] = df.text_updated.apply(get_number_tokens)\n",
    "    df['nb_tok_cs'] = df.nb_tok.cumsum()\n",
    "    # 2. while df is not empty, cut, add (start_index, text) to memory, and loop\n",
    "    nb_splits = np.ceil( df.nb_tok.cumsum().iloc[-1] / max_nb_tokens ) \n",
    "    max_nb_tokens = df.nb_tok.cumsum().iloc[-1] // nb_splits\n",
    "    print(f\"Max number of tokens (speakers everywhere): {df.nb_tok.cumsum().iloc[-1]}; Number of splits: {int(nb_splits)}, Max number of tokens: {max_nb_tokens}\")\n",
    "    while df.shape[0] > 0:\n",
    "        idx_change = df[df['nb_tok_cs'] <= max_nb_tokens].index[-1] + idx_add_after_breakpoint # +1 or +5\n",
    "        #t = df[df['nb_tok_cs'] <= max_nb_tokens] # include the next one too\n",
    "        t = df.loc[:idx_change]\n",
    "        t['speaker_updated'] = (t.speaker != t.speaker.shift())*t.speaker\n",
    "        t['text_updated'] = t.apply(lambda x: f\"<{x.speaker_updated}> {x.text}\" if x.speaker_updated != '' else x.text, axis=1)\n",
    "        d = {'idx_start': t.index[0], 'idx_stop': t.index[-1], 'text': ' '.join(t.text_updated.tolist())}\n",
    "        d['len'] = get_number_tokens(d['text'])\n",
    "        memory.append(d)\n",
    "        # update df\n",
    "        #df = df[df['nb_tok_cs'] >= max_nb_tokens]\n",
    "        df = df.loc[(idx_change+1):]\n",
    "        df['nb_tok_cs'] = df.nb_tok.cumsum()\n",
    "    # 3. return memory\n",
    "    return memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_read_dilemma(s:str):\n",
    "    dilemma = \"trois passagers sont dans une montgolfière qui perd rapidement de l'altitude et qui est en passe de s'écraser le seul moyen pour les passagers de survivre est de sacrifier l'un d'eux parmi ces passagers se trouvent un scientifique dont les recherches pourraient amener au développement d'une thérapie révolutionnaire contre le cancer une maîtresse d'école primaire enceinte et son mari qui est également le pilote de la montgolfière quelle serait la meilleure solution à leur proposer\"\n",
    "    ld = len(dilemma.split())\n",
    "    ss = s.split()\n",
    "    ls = len(ss)\n",
    "    # attempt match\n",
    "    sm = SequenceMatcher()\n",
    "    sm.set_seq1(dilemma)\n",
    "    res = []\n",
    "    for i in range(0, ls - ld, 5):\n",
    "        x = ' '.join(ss[i:i+ld+5])\n",
    "        sm.set_seq2(x)\n",
    "        res.append( {'idx': i, 'lidx': i+ld+5, 'sent': x, 'ratio': sm.ratio()} )\n",
    "    res = pd.DataFrame(res).sort_values('ratio', ascending=False).head(3)\n",
    "    # analyse\n",
    "    if res.ratio.mean() > 0.5:\n",
    "        return int(res.idx.min())\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse response and match to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_chatgpt_res(text:str, mode:str='quotes_needed') -> int: # use with literal_eval\n",
    "    \"\"\"Match themes, quotes based on pattern: [1./ -] (theme)(:|\\( eventual sentence) \"<speaker> quote\"\n",
    "    \"\"\"\n",
    "    auth_modes = ['quotes_needed','all']\n",
    "    if mode not in auth_modes:\n",
    "        raise ValueError(f\"Argument mode must be one of {auth_modes}\")\n",
    "    p = re.compile(r\"\"\"\n",
    "^ # Beginning of line\n",
    "#(?:\\d+\\.|-)[ ] # Bullet or numbered point\n",
    "[tT]hème[ ](?:\\d+:)[ ]\n",
    "(?P<theme>[^(:\\n]+) # Theme of the conversation (any character that isn't `:` or `(`)\n",
    "(\n",
    "  (?::[ ]|[ ]\\(|\\n|\\n[ ]) # Beginning of context (match `: ` or ` (`)\n",
    "  (?P<context>\n",
    "    .*?[ ]? # Context is anything, maybe followed by a space\n",
    "    (?:\"(?P<quote>(.*?))\")? # Quote must be between `\"`\n",
    "  )\n",
    "  \\)?[ ]?\n",
    ")?\n",
    "\\.?\n",
    "$ # End of line\n",
    "\"\"\", re.VERBOSE | re.MULTILINE)\n",
    "\n",
    "    themes = []\n",
    "    quotes = []\n",
    "    for m in p.finditer(text):\n",
    "        theme = m.group(\"theme\")\n",
    "        quote = m.group(\"quote\")\n",
    "        if mode == 'quotes_needed':\n",
    "            if quote is not None:\n",
    "                themes.append((theme, quote))\n",
    "        else:\n",
    "            themes.append(theme)\n",
    "            quotes.append(quote)\n",
    "    if (len(quotes) == 0) or all([q is None for q in quotes]): quotes = None\n",
    "    return themes, quotes\n",
    "\n",
    "def clean_sent(s):\n",
    "    # select sentence only - form <speaker> (need to check if accurate) ... [text] ...\n",
    "    if s[0] == \"<\" and s[3] == \">\":\n",
    "        speaker = s[1:3]\n",
    "        s = s[5:]\n",
    "    else: speaker = None\n",
    "    if s[0:3] == \"...\":\n",
    "        s = s[4:]\n",
    "    if s[-3:] == \"...\":\n",
    "        s = s[:-3]\n",
    "    return s, speaker\n",
    "\n",
    "def match_sentence_loc(s, mem, df):\n",
    "    s, speaker = clean_sent(s) #literal_eval(s)) needed for older version\n",
    "    # check if matching original sentences\n",
    "    #if s in mem['text']:\n",
    "    # method 1: match (consecutive) parts of the sentence to the extract - issue: sole (common) words matching the extract\n",
    "    t = df.loc[mem['idx_start']:(mem['idx_stop']+1)]\n",
    "    if speaker is not None:\n",
    "        t = t[t.speaker == speaker]\n",
    "    t['ratio'] = t.text.apply(lambda x: len(x) if x in s else 0)\n",
    "    sent_start = t.groupby([((t.ratio > 0) != (t.ratio > 0).shift()).cumsum()]).agg({'start': 'min', 'ratio': 'sum'}).reset_index(drop=True).sort_values('ratio', ascending=False).iloc[0]\n",
    "    # issue: partial sentence => fall back to difflib\n",
    "    if sent_start.ratio > 5: # usually long sentences\n",
    "        return sent_start.start\n",
    "    # method 2: use difflib.SequenceMatcher() .set_seq1(s) df.apply(sm.set_seq2(x); sm.ratio()) but issue if the sentence is split()\n",
    "    sm = SequenceMatcher()\n",
    "    sm.set_seq1(s)\n",
    "\n",
    "    def apply_seqmatch(x):\n",
    "        sm.set_seq2(x)\n",
    "        return sm.ratio()\n",
    "    t['ratio'] = t.text.apply(apply_seqmatch)\n",
    "    sent_start = t.sort_values('ratio', ascending=False).iloc[0]\n",
    "    return sent_start.start\n",
    "\n",
    "def match_themes_loc(res:str, mem, df, theme_col:str='theme'):\n",
    "    themes, quotes = parse_chatgpt_res(res)\n",
    "    if quotes is None:\n",
    "        for theme, sent in themes:\n",
    "            sent_start = match_sentence_loc(sent, mem, df)\n",
    "            df.loc[df.start == sent_start, theme_col] = theme\n",
    "    else:\n",
    "        for i, (t, sent) in enumerate(zip(themes, quotes)):\n",
    "            if sent is None:\n",
    "                continue\n",
    "            sent_start = match_sentence_loc(sent, mem, df)\n",
    "            t = f\"theme{i}\" if t is None else t\n",
    "            df.loc[df.start == sent_start, theme_col] = t"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_chatgpt_res(s:str): # use with literal_eval\n",
    "    \"\"\" OLD, updated format :/\n",
    "    Format: \n",
    "    ----------\n",
    "    ```\n",
    "    Thèmes de la conversation:\n",
    "    - [liste ]\n",
    "    \n",
    "    Phrases introduisant les thèmes:\n",
    "    ```\n",
    "\n",
    "    Difficultés:\n",
    "    ----------\n",
    "    * Nombre différent de thèmes et de phrases (dans ce cas les quotes ne sont pas associées à un thème)\n",
    "    * Variations dans la formulation précise des intitulés (`introduisant chaque thème`, `introduisant ces thèmes`)\n",
    "    * Phrases des quotes coupées ==> comment matcher ?\n",
    "    \"\"\"\n",
    "    themes, quotes = s.split('\\n\\n')\n",
    "    themes = themes.split('\\n- ')[1:]\n",
    "    quotes = quotes.split('\\n- ')[1:]\n",
    "    if (len(themes) == len(quotes)) and len(quotes[0].split(':')) >= 2:\n",
    "        return [(t, ': '.join(q.split(': ')[1:])) for t, q in zip(themes, quotes)], None\n",
    "    else:\n",
    "        print(\"Skipping answser, different number of themes and quotes\")\n",
    "        return themes, quotes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calls"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The Los Angeles Dodgers won the World Series in 2020.', 31)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Who won the world series in 2020?\"\n",
    "call_api(s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply on one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for file in transcr_files:\n",
    "    _, fconv = get_conv_df(file)\n",
    "    ann_overlap(fconv)\n",
    "    fconv['is_feedback'] = fconv.overlap + fconv.text.apply(is_feedback)\n",
    "    fconv = fconv[~fconv.is_feedback].reset_index(drop=True)\n",
    "    mem = ' '.join(fconv.text.tolist())\n",
    "    res = (locate_read_dilemma(mem))\n",
    "    #res.append({'file': file, 'idx': mem.idx.tolist(), 'lidx': mem.lidx.max(), 'ratio': mem.ratio.mean(), \n",
    "    #            'std': mem.ratio.std(), 'end': mem.sent.iloc[0][-40:]})\n",
    "    print(file, ' '.join(mem.split()[res:res+30]))\n",
    "#res = pd.DataFrame(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c2a333aed10430f8d25563c084f7242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Which file?', options=('bkt-221116-CGLS.eaf', 'bkt-221117-TFGG.eaf', 'bkt-221118-GDNF.ea…"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_selector = widgets.Dropdown(options = transcr_files, description=\"Which file?\")\n",
    "file_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = file_selector.value\n",
    "dial, fconv = get_conv_df(file)\n",
    "\n",
    "ann_overlap(fconv)\n",
    "fconv['is_feedback'] = fconv.overlap + fconv.text.apply(is_feedback)\n",
    "fconv = fconv[~fconv.is_feedback].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "#res, nb_tok = call_api(t)\n",
    "#res, res2, nb_tok, nb_tok2 # print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of tokens (speakers everywhere): 3758; Number of splits: 2, Max number of tokens: 1879.0\n"
     ]
    }
   ],
   "source": [
    "mem = create_texts(fconv.copy(deep=True), idx_add_after_breakpoint=5)\n",
    "l_speakers = fconv.speaker.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:32<00:00, 16.20s/it]\n"
     ]
    }
   ],
   "source": [
    "calls = {}\n",
    "for i in tqdm(range(len(mem))):\n",
    "    p = create_prompt(mem[i]['text'], speakers = l_speakers)\n",
    "    res, nb_tok = call_api(p)\n",
    "    # currently res, res2\n",
    "    calls[i] = {'split': mem[i], 'prompt': p, 'response': res, 'nb_tokens': nb_tok}\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thème 1: Situation d'urgence (phrase d'introduction: \"trois passagers sont dans une mongolfière qui perd rapidement de latitude et qui est en passe de s'écraser...\")\n",
      "thème 2: Dilemme moral (phrase d'introduction: \"il y en a un qui doit se sacrifier du coup\")\n",
      "thème 3: Possibilité de survie (phrase d'introduction: \"j'aurais peut-être essayé de jeter des sacs deux d'abord très et qui l'ont déjà fait en la fin ce serait la meilleure solution à proposer\")\n",
      "thème 4: Valeurs et morale (phrase d'introduction: \"quand on juge les valeurs d'une ville à clermont\")\n",
      "thème 5: Choix de la victime (phrase d'introduction: \"dans tous les cas je ne serai pas sa je suis la maîtresse d'école eh ben elle a une vie dans son ventre je ne sais pa\")\n",
      "thème 6: Possibilité de volontariat (phrase d'introduction: \"tout ça dépend de moi pourquoi qui je demanderais qui est volontaire d'abord\")\n",
      "thème 7: Alternative au sacrifice (phrase d'introduction: \"ou alors il n'y en a aucun qui ce soir ils décident de mourir tous les trois\")\n"
     ]
    }
   ],
   "source": [
    "print(calls[0]['response'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving responses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_path = os.path.expanduser('~/Downloads/chatgpt-calls.json')\n",
    "if os.path.exists(mem_path):\n",
    "    with open(mem_path, 'r') as f:\n",
    "        mem_global = json.load(f)\n",
    "else:\n",
    "    mem_global = {}\n",
    "\n",
    "mem_global[file] = calls\n",
    "with open(mem_path, 'w') as f:\n",
    "    json.dump(mem_global, f, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parse responses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"Situation d'urgence\",\n",
       "  'Dilemme moral',\n",
       "  'Possibilité de survie',\n",
       "  'Valeurs et morale',\n",
       "  'Choix de la victime',\n",
       "  'Possibilité de volontariat',\n",
       "  'Alternative au sacrifice'],\n",
       " [\"trois passagers sont dans une mongolfière qui perd rapidement de latitude et qui est en passe de s'écraser...\",\n",
       "  'il y en a un qui doit se sacrifier du coup',\n",
       "  \"j'aurais peut-être essayé de jeter des sacs deux d'abord très et qui l'ont déjà fait en la fin ce serait la meilleure solution à proposer\",\n",
       "  \"quand on juge les valeurs d'une ville à clermont\",\n",
       "  \"dans tous les cas je ne serai pas sa je suis la maîtresse d'école eh ben elle a une vie dans son ventre je ne sais pa\",\n",
       "  \"tout ça dépend de moi pourquoi qui je demanderais qui est volontaire d'abord\",\n",
       "  \"ou alors il n'y en a aucun qui ce soir ils décident de mourir tous les trois\"])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_chatgpt_res(calls[0]['response'], 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "fconv['theme'] = None\n",
    "for i in range(len(calls)):\n",
    "    res = calls[i]['response']\n",
    "    memo = calls[i]['split']\n",
    "    match_themes_loc(res, memo, fconv)\n",
    "fconv['theme_filled'] = fconv.theme.ffill()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving DF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = os.path.expanduser('~/Downloads/chatgpt-themes-brainkt.csv')\n",
    "fcols = fconv.columns.tolist()\n",
    "fpattern = file.replace('-','_')[5:-4]\n",
    "fconv['file'] = fpattern\n",
    "fconv = fconv[['file']+fcols] # reordering columns\n",
    "if os.path.exists(csv_path):\n",
    "    fdf = pd.read_csv(csv_path, na_values=[''])\n",
    "    if fpattern in fdf.file.unique():\n",
    "        fdf = fdf[fdf.file != fpattern]\n",
    "    fdf = pd.concat([fdf, fconv], axis=0).reset_index(drop=True)\n",
    "else:\n",
    "    fdf = fconv\n",
    "fdf.to_csv(csv_path, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For previous files update:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing files: 0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "mf = sorted(list(set([f.replace('-','_')[5:-4] for f in transcr_files]) - set(fdf.file.unique().tolist())))\n",
    "print(f\"Missing files: {len(mf)}\")\n",
    "print(mf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b4a7edb5554f02a3365474a5e79cb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Which file?', options=('bkt-221116-CGLS.eaf', 'bkt-221117-TFGG.eaf', 'bkt-221118-VPET.ea…"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redo_files = list(mem_global.keys())\n",
    "frev_selector = widgets.Dropdown(options = redo_files, description=\"Which file?\")\n",
    "frev_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = frev_selector.value\n",
    "calls = mem_global[file]\n",
    "calls = {int(k):v for k,v in calls.items()}\n",
    "# recompute fconv\n",
    "dial, fconv = get_conv_df(file)\n",
    "ann_overlap(fconv)\n",
    "fconv['is_feedback'] = fconv.overlap + fconv.text.apply(is_feedback)\n",
    "fconv = fconv[~fconv.is_feedback].reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "* Number of themes / file\n",
    "* Number of manual themes (aggregated)\n",
    "* Duration (ipus/time) of dilemma / themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing for manual review\n",
    "# for each file - grab from mem_global[file]['response'] the themes (esp if some are not annotated)\n",
    "# for each file - groupby shift().cumsum() => join list => \n",
    "for file in mem_global.keys():\n",
    "    file_pattern = file.replace('-','_')[5:-4]\n",
    "    if file_pattern in fdf.file.unique():\n",
    "        print(\"\\n\\n\", file, end='\\n---------------\\n')\n",
    "        print('\\n'.join(mem_global[file][x]['response'] for x in sorted(mem_global[file])))\n",
    "        t = fdf[fdf.file == file_pattern]\n",
    "        t['speaker_updated'] = (t.speaker != t.speaker.shift())*t.speaker\n",
    "        t['text_updated'] = t.apply(lambda x: f\"<{x.speaker_updated}> {x.text}\" if x.speaker_updated != '' else x.text, axis=1)\n",
    "        tg = t.groupby((t['theme_filled'].fillna('None') != t['theme_filled'].fillna('None').shift()).cumsum()).agg(\n",
    "            {'theme_filled':lambda x: list(x)[0], 'text_updated': lambda x: ' '.join(list(x))}).set_index('theme_filled')\n",
    "        for theme, row in tg.iterrows():\n",
    "            print('-------', theme, '-------')\n",
    "            print(row.text_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>theme_idx</th>\n",
       "      <th>theme</th>\n",
       "      <th>nb_ipus</th>\n",
       "      <th>start</th>\n",
       "      <th>stop</th>\n",
       "      <th>duration</th>\n",
       "      <th>first_spk</th>\n",
       "      <th>dur_spk1</th>\n",
       "      <th>dur_spk2</th>\n",
       "      <th>ratio_spk1</th>\n",
       "      <th>ratio_spk2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21116_CGLS</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>1018.305</td>\n",
       "      <td>1343.192</td>\n",
       "      <td>324.887</td>\n",
       "      <td>spk1</td>\n",
       "      <td>99.385</td>\n",
       "      <td>113.129</td>\n",
       "      <td>0.467663</td>\n",
       "      <td>0.532337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21116_CGLS</td>\n",
       "      <td>2</td>\n",
       "      <td>Choix de la personne à jeter d'une montgolfière</td>\n",
       "      <td>51</td>\n",
       "      <td>1346.085</td>\n",
       "      <td>1491.300</td>\n",
       "      <td>145.215</td>\n",
       "      <td>spk2</td>\n",
       "      <td>50.361</td>\n",
       "      <td>55.848</td>\n",
       "      <td>0.474169</td>\n",
       "      <td>0.525831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         file  theme_idx                                            theme  \\\n",
       "0  21116_CGLS          1                                              NaN   \n",
       "1  21116_CGLS          2  Choix de la personne à jeter d'une montgolfière   \n",
       "\n",
       "   nb_ipus     start      stop  duration first_spk  dur_spk1  dur_spk2  \\\n",
       "0      100  1018.305  1343.192   324.887      spk1    99.385   113.129   \n",
       "1       51  1346.085  1491.300   145.215      spk2    50.361    55.848   \n",
       "\n",
       "   ratio_spk1  ratio_spk2  \n",
       "0    0.467663    0.532337  \n",
       "1    0.474169    0.525831  "
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theme_col = 'theme_filled'\n",
    "fdf_an = []\n",
    "for idx, group in fdf.groupby(['file', (fdf[theme_col].fillna('None') != fdf[theme_col].fillna('None').shift()).cumsum()]):\n",
    "    spk_1 = idx[0][-4:-2]\n",
    "    d = {\n",
    "        'file': idx[0], 'theme_idx': idx[1], 'theme': group[theme_col].iloc[0], 'nb_ipus': group.shape[0], \n",
    "        'start': group.start.iloc[0], 'stop': group.stop.iloc[-1], 'duration': group.stop.iloc[-1] - group.start.iloc[0],\n",
    "        'first_spk': f'spk{2-(group.speaker.iloc[0] == spk_1)}',\n",
    "        'dur_spk1': group[group.speaker == spk_1].duration.astype(float).sum(), \n",
    "        'dur_spk2': group[group.speaker != spk_1].duration.astype(float).sum(), \n",
    "    }\n",
    "    for i in [1,2]: \n",
    "        d[f'ratio_spk{i}'] = d[f'dur_spk{i}'] / (d['dur_spk1']+d['dur_spk2'])\n",
    "    fdf_an.append(d)\n",
    "fdf_an = pd.DataFrame(fdf_an)\n",
    "fdf_an.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>theme_idx</th>\n",
       "      <th>duration</th>\n",
       "      <th>dur_spk1</th>\n",
       "      <th>dur_spk2</th>\n",
       "      <th>ratio_spk1</th>\n",
       "      <th>ratio_spk2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>file</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21116_CGLS</th>\n",
       "      <td>8</td>\n",
       "      <td>108.454625</td>\n",
       "      <td>266.406</td>\n",
       "      <td>388.403</td>\n",
       "      <td>0.406845</td>\n",
       "      <td>0.593155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21117_TFGG</th>\n",
       "      <td>12</td>\n",
       "      <td>74.090667</td>\n",
       "      <td>139.584</td>\n",
       "      <td>362.688</td>\n",
       "      <td>0.277905</td>\n",
       "      <td>0.722095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            theme_idx    duration  dur_spk1  dur_spk2  ratio_spk1  ratio_spk2\n",
       "file                                                                         \n",
       "21116_CGLS          8  108.454625   266.406   388.403    0.406845    0.593155\n",
       "21117_TFGG         12   74.090667   139.584   362.688    0.277905    0.722095"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdf_anf = fdf_an.groupby('file').agg({'theme_idx': lambda x: len(list(x)), 'duration': 'mean', 'dur_spk1': 'sum', 'dur_spk2': 'sum'})\n",
    "for i in [1,2]:\n",
    "    fdf_anf.loc[:,f'ratio_spk{i}'] = fdf_anf[f'dur_spk{i}']/(fdf_anf.dur_spk1+fdf_anf.dur_spk2)\n",
    "fdf_anf.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "**Difficulties**: various ways to answer depending on days/number of calls?\n",
    "\n",
    "23/04/25:\n",
    "```\n",
    "-------- prompt 1\n",
    "Thèmes de la conversation: \n",
    "[4 themes]\n",
    "\n",
    "Phrases introduisant ces thèmes:\n",
    "[6 sentences]\n",
    "\n",
    "-------- prompt 2\n",
    "Thèmes de la conversation: \n",
    "- Choix difficile à faire dans une situation hypothétique (sauter d'une montgolfière)\n",
    "- Orientation professionnelle et projets d'avenir\n",
    "- Sujet de recherche en doctorat et expériences à mettre en place\n",
    "\n",
    "Phrases introduisant chaque thème:\n",
    "- Choix difficile: \"<CG> Après s'ils sont pas très haut, qu'il y en a un qui est un petit peu adroit de ses jambes, il saute et il se réceptionne...\"\n",
    "- Orientation professionnelle: \"<CG> Et du coup, tu fais quoi, toi? T'es en... en licence? <LS> Alors euh... j'ai une licence en management et je me réoriente, j'avais commencé un master en achat en école de commerce et je me réoriente dans le sport et euh... j'ai passé de concours pour être moniteur de sport dans l'armée de l'air...\"\n",
    "- Sujet de recherche: \"<CG> euh... Mon sujet c'est euh... la... l'adaptation du langage à l'adolescence...\"\n",
    "```\n",
    "\n",
    "23/04/26:\n",
    "```\n",
    "-------- prompt 1\n",
    "Thèmes de la conversation :\n",
    "1. Le dilemme de la mongolfière qui perd de l'altitude (début de la conversation avec la lecture du dilemme)\n",
    "2. Les différentes solutions envisageables pour résoudre le dilemme (proposition d'essayer de poser la mongolfière, balance pour peser les passagers, etc.)\n",
    "3. Le choix difficile à faire entre la vie du scientifique et celle du couple (discussion sur l'utilité de chaque passager, proposition de sacrifier personne, etc.)\n",
    "4. La réflexion sur la situation elle-même (contexte, possibilité d'autres solutions, etc.)\n",
    "\n",
    "-------- prompt 2\n",
    "Les différents thèmes de la conversation sont :\n",
    "\n",
    "- Choix de la personne à jeter d'une montgolfière (débutant avec la phrase \"<LS> c'est un scientifique merde\").\n",
    "- Orientation professionnelle et études, expériences professionnelles passées (débutant avec la phrase \"<CG> Et du coup, tu fais quoi, toi? T'es en... en licence?\").\n",
    "- Sujet de recherche en doctorat et expériences à mettre en place (débutant avec la phrase \"<CG> euh... Mon sujet c'est euh... la... l'adaptation du langage à l'adolescence\").\n",
    "```\n",
    "\n",
    "Variations in: the way to deliver the information. However, the _content_ hasn't changed much: same number of themes (and inability to give sentences) for prompt1, 3 themes with about the same sentences for prompt2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with other methods: TextTiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TextTilingTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_tt(s:str, ttt = TextTilingTokenizer()):\n",
    "    \"\"\"Analyse a conversation using TextTiling to generate topics segmentation.\n",
    "    Then retrace to initial conversation using '\\n' as utterance breaks.\n",
    "    Returns a dict and a list: the dict contains a list of utterances associated with each topic, \n",
    "    the list containing the index of the topic for each utterance (shape: df[df.file == X].shape[0])\n",
    "    \"\"\"\n",
    "    d_utter = {}\n",
    "    l_idx = []\n",
    "    tokens = ttt.tokenize(s)\n",
    "    for i, token in enumerate(tokens):\n",
    "        tok_utter = token.split(\"\\n\\n\")\n",
    "        tok_utter = [x for x in tok_utter if x != ''] # remove ' from list\n",
    "        d_utter[i] = tok_utter\n",
    "        l_idx += [i]*len(tok_utter)\n",
    "\n",
    "    return d_utter, l_idx\n",
    "\n",
    "d_utter, l_idx = one_tt('\\n\\n'.join(fconv.text.tolist()))\n",
    "assert(fconv.shape[0] == len(l_idx))\n",
    "fconv['TextTiling'] = l_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['speaker', 'start', 'stop', 'duration', 'text', 'overlap',\n",
       "       'is_feedback', 'TextTiling'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fconv.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, group in fconv.groupby('TextTiling'):\n",
    "    group = group[~group.is_feedback]\n",
    "    group['speaker_updated'] = (group.speaker != group.speaker.shift())*group.speaker\n",
    "    group['text_updated'] = group.apply(lambda x: f\"<{x.speaker_updated}> {x.text}\" if x.speaker_updated != '' else x.text, axis=1)\n",
    "    print(' '.join(group.text_updated.tolist()))\n",
    "# very short themes, chatgpt are longer (slightly)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding themes annotation to eaf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
